{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.preprocessing import DataPreprocessing\n",
    "from src.data_models import DataModels\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmor3no\u001b[0m (\u001b[33mteam-ss\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/raw\"\n",
    "DATE = \"_06_06_22\"\n",
    "PURPLE_PATH = f\"{DATA_DIR}/purple{DATE}.csv\"\n",
    "AIRE_PATH = f\"{DATA_DIR}/aire{DATE}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataPreprocessing(PURPLE_PATH, AIRE_PATH)\n",
    "dp.preprocess()\n",
    "purple, aire = dp.get_data()\n",
    "data = DataModels(purple, aire)\n",
    "data.create_hour_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "purple, aire = data.get_data()\n",
    "X = purple.values\n",
    "y = aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28359, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28359,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21269, 5])\n",
      "torch.Size([21269, 1])\n",
      "torch.Size([7090, 5])\n"
     ]
    }
   ],
   "source": [
    "## Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(purple, aire)\n",
    "X_train = torch.FloatTensor(X_train.values)\n",
    "X_test = torch.FloatTensor(X_test.values)\n",
    "y_train = torch.FloatTensor(y_train).view(-1,1)\n",
    "y_test = torch.FloatTensor(y_test).view(-1,1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN: Ecuación de correción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam']\n",
    "        },\n",
    "    'fc_layer_size': {\n",
    "        'values': [128, 256, 512]\n",
    "        },\n",
    "    'dropout': {\n",
    "          'values': [0.3, 0.4, 0.5]\n",
    "        },\n",
    "    'batch_size': {\n",
    "          'values': [512, 1024]\n",
    "        },\n",
    "    'learning_rate': {\n",
    "          'values': [ 0.01, 0.02]\n",
    "        },\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict.update({\n",
    "    'epochs': {\n",
    "        'value': 50}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: iejpknwg\n",
      "Sweep URL: https://wandb.ai/team-ss/cei-sweeps-demo/sweeps/iejpknwg\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"cei-sweeps-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers_sz):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = list()\n",
    "        \n",
    "        for idx in range(len(layers_sz)-2):\n",
    "            layers.append(nn.Linear(layers_sz[idx], layers_sz[idx+1]))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        layers.append(nn.Linear(layers_sz[-2], layers_sz[-1]))\n",
    "        \n",
    "        self.layers_nn = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, X):\n",
    "\n",
    "        out = self.layers_nn(X)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "def build_dataset(batch_size):\n",
    "    \n",
    "    dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def build_network(fc_layer_size):\n",
    "    network = TabularModel([5, fc_layer_size, fc_layer_size, 1])\n",
    "    return network\n",
    "\n",
    "    \n",
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train_epoch(network, loader, optimizer):\n",
    "    cumu_loss = 0\n",
    "    for idx, (data, target) in enumerate(loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ➡ Forward pass\n",
    "        y_pred = network(data)\n",
    "        loss = criterion(y_pred, target)\n",
    "        cumu_loss += loss\n",
    "        # ⬅ Backward pass + weight update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #wandb.log({\"train batch loss\": loss})\n",
    "        with torch.no_grad():\n",
    "            y_pred = network(X_test)\n",
    "            val_loss = criterion(y_pred, y_test)\n",
    "            wandb.log({\"val loss\": val_loss.item()})\n",
    "\n",
    "    return cumu_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        loader = build_dataset(config.batch_size)\n",
    "        network = build_network(config.fc_layer_size)\n",
    "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
    "\n",
    "        for epoch in range(config.epochs):            \n",
    "            avg_loss = train_epoch(network, loader, optimizer)\n",
    "            wandb.log({\"loss\": avg_loss, \"epoch\": epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "855fc1574b612edc152e788a18e6bfd6eef2989cd58aa07962c4162092fd7caf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
